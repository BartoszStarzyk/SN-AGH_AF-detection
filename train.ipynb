{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# TODO:\n",
        "# DICE loss\n",
        "# sprawdzenie na innym datasecie\n",
        "# przeniesienie do plików py\n",
        "# uprzątnięcie repo\n",
        "# dokumentacja\n",
        "# interfejs\n",
        "# Pasek do trenowania"
      ],
      "metadata": {
        "id": "BPATWzFEyNVQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install wfdb"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WxrY7dfv9Twx",
        "outputId": "47d07855-c18e-41d9-ec7a-ef90aeb83efb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting wfdb\n",
            "  Downloading wfdb-4.1.2-py3-none-any.whl.metadata (4.3 kB)\n",
            "Requirement already satisfied: SoundFile>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from wfdb) (0.12.1)\n",
            "Requirement already satisfied: matplotlib>=3.2.2 in /usr/local/lib/python3.10/dist-packages (from wfdb) (3.8.0)\n",
            "Requirement already satisfied: numpy>=1.10.1 in /usr/local/lib/python3.10/dist-packages (from wfdb) (1.26.4)\n",
            "Requirement already satisfied: pandas>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from wfdb) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from wfdb) (2.32.3)\n",
            "Requirement already satisfied: scipy>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from wfdb) (1.13.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.2.2->wfdb) (1.3.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.2.2->wfdb) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.2.2->wfdb) (4.54.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.2.2->wfdb) (1.4.7)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.2.2->wfdb) (24.1)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.2.2->wfdb) (10.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.2.2->wfdb) (3.2.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.2.2->wfdb) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.3.0->wfdb) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.3.0->wfdb) (2024.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.8.1->wfdb) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.8.1->wfdb) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.8.1->wfdb) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.8.1->wfdb) (2024.8.30)\n",
            "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.10/dist-packages (from SoundFile>=0.10.0->wfdb) (1.17.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.0->SoundFile>=0.10.0->wfdb) (2.22)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib>=3.2.2->wfdb) (1.16.0)\n",
            "Downloading wfdb-4.1.2-py3-none-any.whl (159 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m160.0/160.0 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: wfdb\n",
            "Successfully installed wfdb-4.1.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_lhHyXHl7Vbr"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import wfdb\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from sklearn.metrics import accuracy_score"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/drive/MyDrive/Stopień_2/Semestr_2/DNN\n",
        "# Define parameters\n",
        "data_dir = 'files'  # Directory containing the records\n",
        "sequence_length = 3000  # Sequence length (in samples)\n",
        "# fs = 360  # Sampling frequency of MIT-BIH (use 360 Hz)\n",
        "batch_size = 32\n",
        "num_epochs = 10\n",
        "learning_rate = 0.001"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eMQr0AAl9W0Z",
        "outputId": "13400918-027b-4a43-af92-c928def39e7c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/Stopień_2/Semestr_2/DNN\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Custom Dataset for ECG data\n",
        "class ECGDataset(Dataset):\n",
        "    def __init__(self, signals, labels):\n",
        "        self.signals = signals\n",
        "        self.labels = labels\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        signal = self.signals[idx]\n",
        "        label = self.labels[idx]\n",
        "        return torch.tensor(signal, dtype=torch.float32), torch.tensor(label, dtype=torch.uint8)"
      ],
      "metadata": {
        "id": "Fq2sR7wp94bn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_label(ann, signal_len):\n",
        "    '''Create a label containing ones where AFIB is present and zeros elsewhere'''\n",
        "    # create label placeholder\n",
        "    label = np.zeros(signal_len, dtype=np.uint8)\n",
        "    aux_notes = ann.aux_note\n",
        "    # add signal length to samples to make it easier to iterate\n",
        "    samples = np.append(ann.sample, signal_len)\n",
        "    # add end to aux_notes to make it easier to iterate\n",
        "    aux_notes.append('END')\n",
        "\n",
        "    start = 0\n",
        "    is_afib = False\n",
        "    for i, note in enumerate(aux_notes):\n",
        "        if note == '(AFIB':\n",
        "            is_afib = True\n",
        "            start = samples[i]\n",
        "        else:\n",
        "            end = samples[i]\n",
        "            if is_afib:\n",
        "                label[start:end] = 1\n",
        "    return label"
      ],
      "metadata": {
        "id": "w_W8d2AOQywc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Feature extraction and data loading\n",
        "def load_data(record_names, af_symbols=['(AFIB'], segment_length=sequence_length):\n",
        "    X = []\n",
        "    y = []\n",
        "\n",
        "    for record_name in record_names:\n",
        "        # Load the ECG signal and annotation\n",
        "        print(record_name)\n",
        "        record_path = os.path.join(data_dir, record_name)\n",
        "        signal, fields = wfdb.rdsamp(record_path)\n",
        "        annotation = wfdb.rdann(record_path, 'atr')\n",
        "\n",
        "        # Extract the first channel (ECG signal)\n",
        "        ecg_signal = signal[:, 0]\n",
        "        ecg_label = create_label(annotation, len(ecg_signal))\n",
        "\n",
        "        # Segment the ECG signal into fixed-length windows\n",
        "        num_segments = len(ecg_signal) // segment_length\n",
        "        for i in range(num_segments):\n",
        "            segment_start = i * segment_length\n",
        "            segment_end = (i + 1) * segment_length\n",
        "            segment = ecg_signal[segment_start:segment_end]\n",
        "            label = ecg_label[segment_start:segment_end]\n",
        "            X.append(segment)\n",
        "            y.append(label)\n",
        "\n",
        "    return np.array(X), np.array(y)"
      ],
      "metadata": {
        "id": "ScQSZhod98ZO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load data\n",
        "excluded_records = ['00735', '03665']\n",
        "db_path = './files/'\n",
        "record_names = np.loadtxt(db_path + 'RECORDS', dtype=str)\n",
        "record_names = [name for name in record_names if name not in excluded_records]\n",
        "X, y = load_data(record_names)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "anWLQYjh-Brq",
        "outputId": "6a297d52-ac14-49a7-8724-0598ba0c1e3e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "04015\n",
            "04043\n",
            "04048\n",
            "04126\n",
            "04746\n",
            "04908\n",
            "04936\n",
            "05091\n",
            "05121\n",
            "05261\n",
            "06426\n",
            "06453\n",
            "06995\n",
            "07162\n",
            "07859\n",
            "07879\n",
            "07910\n",
            "08215\n",
            "08219\n",
            "08378\n",
            "08405\n",
            "08434\n",
            "08455\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Scale the data\n",
        "scaler = StandardScaler()\n",
        "X = scaler.fit_transform(X.reshape(-1, sequence_length)).reshape(-1, 1, sequence_length)"
      ],
      "metadata": {
        "id": "HUZ1J0-5_a8o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "qkaMvKEt_fIt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create DataLoaders\n",
        "train_dataset = ECGDataset(X_train, y_train)\n",
        "test_dataset = ECGDataset(X_test, y_test)\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
      ],
      "metadata": {
        "id": "mQyKRGgc_ijK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class UNet1D(nn.Module):\n",
        "    def __init__(self, in_channels=1, out_channels=1, init_features=32):\n",
        "        super(UNet1D, self).__init__()\n",
        "        features = init_features\n",
        "\n",
        "        # Encoder\n",
        "        self.encoder1 = UNet1D._block(in_channels, features, name=\"enc1\")\n",
        "        self.pool1 = nn.MaxPool1d(kernel_size=2, stride=2)\n",
        "        self.encoder2 = UNet1D._block(features, features * 2, name=\"enc2\")\n",
        "        self.pool2 = nn.MaxPool1d(kernel_size=2, stride=2)\n",
        "        self.encoder3 = UNet1D._block(features * 2, features * 4, name=\"enc3\")\n",
        "        self.pool3 = nn.MaxPool1d(kernel_size=2, stride=2)\n",
        "        self.encoder4 = UNet1D._block(features * 4, features * 8, name=\"enc4\")\n",
        "        self.pool4 = nn.MaxPool1d(kernel_size=2, stride=2)\n",
        "\n",
        "        # Bottleneck\n",
        "        self.bottleneck = UNet1D._block(features * 8, features * 16, name=\"bottleneck\")\n",
        "\n",
        "        # Decoder\n",
        "        self.upconv4 = nn.ConvTranspose1d(features * 16, features * 8, kernel_size=2, stride=2)\n",
        "        self.decoder4 = UNet1D._block(features * 16, features * 8, name=\"dec4\")\n",
        "        self.upconv3 = nn.ConvTranspose1d(features * 8, features * 4, kernel_size=2, stride=2)\n",
        "        self.decoder3 = UNet1D._block(features * 8, features * 4, name=\"dec3\")\n",
        "        self.upconv2 = nn.ConvTranspose1d(features * 4, features * 2, kernel_size=2, stride=2)\n",
        "        self.decoder2 = UNet1D._block(features * 4, features * 2, name=\"dec2\")\n",
        "        self.upconv1 = nn.ConvTranspose1d(features * 2, features, kernel_size=2, stride=2)\n",
        "        self.decoder1 = UNet1D._block(features * 2, features, name=\"dec1\")\n",
        "\n",
        "        # Output layer\n",
        "        self.conv = nn.Conv1d(features, out_channels, kernel_size=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Encoder\n",
        "        enc1 = self.encoder1(x)\n",
        "        enc2 = self.encoder2(self.pool1(enc1))\n",
        "        enc3 = self.encoder3(self.pool2(enc2))\n",
        "        enc4 = self.encoder4(self.pool3(enc3))\n",
        "\n",
        "        # Bottleneck\n",
        "        bottleneck = self.bottleneck(self.pool4(enc4))\n",
        "\n",
        "        # Decoder\n",
        "        dec4 = self.upconv4(bottleneck)\n",
        "        if dec4.size(-1) != enc4.size(-1):\n",
        "            enc4 = enc4[:, :, :dec4.size(-1)]\n",
        "        dec4 = torch.cat((dec4, enc4), dim=1)\n",
        "        dec4 = self.decoder4(dec4)\n",
        "\n",
        "        dec3 = self.upconv3(dec4)\n",
        "        if dec3.size(-1) != enc3.size(-1):\n",
        "            enc3 = enc3[:, :, :dec3.size(-1)]\n",
        "        dec3 = torch.cat((dec3, enc3), dim=1)\n",
        "        dec3 = self.decoder3(dec3)\n",
        "\n",
        "        dec2 = self.upconv2(dec3)\n",
        "        if dec2.size(-1) != enc2.size(-1):\n",
        "            enc2 = enc2[:, :, :dec2.size(-1)]\n",
        "        dec2 = torch.cat((dec2, enc2), dim=1)\n",
        "        dec2 = self.decoder2(dec2)\n",
        "\n",
        "        dec1 = self.upconv1(dec2)\n",
        "        if dec1.size(-1) != enc1.size(-1):\n",
        "            enc1 = enc1[:, :, :dec1.size(-1)]\n",
        "        dec1 = torch.cat((dec1, enc1), dim=1)\n",
        "        dec1 = self.decoder1(dec1)\n",
        "\n",
        "        # Output layer\n",
        "        output = self.conv(dec1)\n",
        "\n",
        "        return torch.sigmoid(output)\n",
        "\n",
        "    @staticmethod\n",
        "    def _block(in_channels, features, name):\n",
        "        return nn.Sequential(\n",
        "            nn.Conv1d(in_channels, features, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv1d(features, features, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )"
      ],
      "metadata": {
        "id": "FLkzRSrU_l4m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = UNet1D(in_channels=1, out_channels=1).to(device)\n",
        "criterion = nn.BCELoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)"
      ],
      "metadata": {
        "id": "WCblNUv9_mvU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('Training')\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    for i, (signals, labels) in enumerate(train_loader):\n",
        "        signals, labels = signals.to(device), labels.to(device).float()  # Ensure float labels\n",
        "        labels = labels.unsqueeze(1)\n",
        "        # Forward pass\n",
        "        outputs = model(signals)\n",
        "\n",
        "        # Ensure output and labels match in shape\n",
        "\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        # Backward pass and optimization\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "\n",
        "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss / len(train_loader):.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-e1Wn07R_oox",
        "outputId": "cc7d83ba-fc92-49b6-f8bc-b77747817c70"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training\n",
            "Epoch [1/10], Loss: 0.6668\n",
            "Epoch [2/10], Loss: 0.6075\n",
            "Epoch [3/10], Loss: 0.3976\n",
            "Epoch [4/10], Loss: 0.2929\n",
            "Epoch [5/10], Loss: 0.2490\n",
            "Epoch [6/10], Loss: 0.2255\n",
            "Epoch [7/10], Loss: 0.2109\n",
            "Epoch [8/10], Loss: 0.2001\n",
            "Epoch [9/10], Loss: 0.1920\n",
            "Epoch [10/10], Loss: 0.1820\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluation\n",
        "torch.save(model.state_dict(), 'modelunet11.pt')\n",
        "model.load_state_dict(torch.load('modelunet11.pt'))\n",
        "y_pred = []\n",
        "y_true = []\n",
        "with torch.no_grad():\n",
        "    for signals, labels in test_loader:\n",
        "        signals, labels = signals.to(device), labels.to(device).float()\n",
        "        outputs = model(signals)\n",
        "\n",
        "        # Threshold predictions\n",
        "        predicted = (outputs > 0.5).float()\n",
        "\n",
        "        y_pred.extend(predicted.cpu().numpy().flatten())\n",
        "        y_true.extend(labels.cpu().numpy().flatten())\n",
        "\n",
        "# Accuracy\n",
        "accuracy = accuracy_score(y_true, y_pred)\n",
        "print(\"Test Accuracy:\", accuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BistljqZ_rHw",
        "outputId": "7deb91f2-4a32-40f7-f334-dde484e7eafe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-63-40fde2fd475d>:3: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model.load_state_dict(torch.load('modelunet11.pt'))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy: 0.9233489149768765\n"
          ]
        }
      ]
    }
  ]
}